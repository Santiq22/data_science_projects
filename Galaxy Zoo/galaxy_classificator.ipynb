{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fed3ea7",
   "metadata": {},
   "source": [
    "## Deep Convolutional Classificator using Galaxy Zoo dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cd66f",
   "metadata": {},
   "source": [
    "![](ngc2841.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a479b46e",
   "metadata": {},
   "source": [
    "In this project, we will make a convolutional neural network (hereafter CNN) to classify the different astronomical images contained on the [Galaxy Zoo dataset](https://www.kaggle.com/competitions/galaxy-zoo-the-galaxy-challenge/overview). This set has 61.578 training images and 79.975 test images, which will be used to train and test the deep convolutional classificator. The idea is to analyze the JPG images of galaxies to find automated metrics that reproduce the probability distributions derived from human classifications. For each galaxy, determine the probability that it belongs in a particular class.\n",
    "\n",
    "In July 2007, astronomers from Oxford University had in their possession a data set of approximately 1 million galaxies imaged by the Sloan Digital Sky Survey (SDSS). But the galaxies in this data set needed to have their morphologies (shapes) classified in order to be used to better understand galactic processes. With so many galaxies, it would have taken an individual a thousand lifetimes to classify all of them. Instead, [Galaxy Zoo was born](https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/).\n",
    "\n",
    "Galaxy Zoo took an innovative approach that brought astronomy to the general public: log on and help classify a galaxy, driven to a successful citizen science crowdsourcing project.\n",
    "\n",
    "It was initially assumed that despite outsourcing the work to thousands in the general public, it would still take years for all of the images to be classified. Within the first 24 hours of launch, Galaxy Zoo founders were stunned to be receiving nearly 70 000 classifications an hour. In the end, more than 50 million classifications were received by the project during its first year, contributed by more than 150 000 people.\n",
    "\n",
    "The Galaxy Zoo project has gone through four iterations. The first was focused on deciding if a galaxy was elliptical, spiral (including direction), or a merger of two galaxies.\n",
    "\n",
    "*Galaxy Zoo 2* asked for more details on bright SDSS galaxies. These detailed classifications include (amongst others) measurements of the bulge size, presence of bars, and the structure of spiral arms. This is what it is aimed in this project, to construct a machine learning algorithm able to classify a given galaxy based on the classification rules defined in Galaxy Zoo 2. These rules are described in the image below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"Figures/decision_tree_of_classifications.png\" style=\"width:80%\">\n",
    "</figure>\n",
    "\n",
    "##### Weighting the responses\n",
    "\n",
    "The values of the morphology categories in the solution file are computed as follows. For the first set of responses (smooth, features/disk, star/artifact), the values in each category are simply the likelihood of the galaxy falling in each category. These values sum to 1.0. For each subsequent question, the probabilities are first computed (these will sum to 1.0) and then multiplied by the value which led to that new set of responses. \n",
    "\n",
    "Here is a simplified example: a galaxy had 80% of users identify it as smooth, 15% as having features/disk, and 5% as a star/artifact.\n",
    "\n",
    "| Class | Probability of being that class |\n",
    "| :---: | :---: |\n",
    "| Class1.1 | 0.80 |\n",
    "| Class1.2 | 0.15 |\n",
    "| Class1.3 | 0.05 |\n",
    "\n",
    "For the 80% of users that identified the galaxy as \"smooth\", they also recorded responses for the galaxy's relative roundness. These votes were for 50% completely round, 25% in-between, and 25% cigar-shaped. The values in the solution file are thus:\n",
    "\n",
    "| Class | Weighted probability of being that class |\n",
    "| :---: | :---: |\n",
    "| Class 7.1 | 0.80 * 0.50 = 0.40 |\n",
    "| Class 7.2 | 0.80 * 0.25 = 0.20 |\n",
    "| Class 7.3 | 0.80 * 0.25 = 0.20 |\n",
    "\n",
    "The reason for this weighting is to emphasize that a good solution must get the high-level, large-scale morphology categories correct. The best solutions, though, will also have high levels of accuracy on the detailed solutions that are further down the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a1618",
   "metadata": {},
   "source": [
    "| Question | Number of possible answers |\n",
    "| --- | --- |\n",
    "| Q1. Is the object a smooth galaxy, a galaxy with features/disk or a star? | 3 responses |\n",
    "| Q2. Is it edge-on? | 2 responses |\n",
    "| Q3. Is there a bar? | 2 responses |\n",
    "| Q4. Is there a spiral pattern? | 2 responses |\n",
    "| Q5. How prominent is the central bulge? | 4 responses |\n",
    "| Q6. Is there anything \"odd\" about the galaxy? | 2 responses |\n",
    "| Q7. How round is the smooth galaxy? | 3 responses |\n",
    "| Q8. What is the odd feature? | 7 responses |\n",
    "| Q9. What shape is the bulge in the edge-on galaxy? | 3 responses |\n",
    "| Q10. How tightly wound are the spiral arms? | 3 responses |\n",
    "| Q11. How many spiral arms are there? | 6 responses |\n",
    "\n",
    "As a result, at each node or question, the total initial probability of a classification will sum to 1.0.\n",
    "\n",
    "We will implement a CNN whose aim is to predict the type of galaxy based on the above table. Let us start importing the packages we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d0b16c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Torch tools\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "# --- Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "\n",
    "# --- Data analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "#import sklearn.metrics as met\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fedf6b",
   "metadata": {},
   "source": [
    "Before we proceed, we will go through a brief explanation of what is a CNN.\n",
    "\n",
    "As it is said in this [IBM article](https://www.ibm.com/topics/convolutional-neural-networks), neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold, as it is shown below.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Figures/neural_networks-001.png\" style=\"width:80%\">\n",
    "    <figcaption>Source: https://tikz.net/neural_networks/</figcaption>\n",
    "</figure>\n",
    "\n",
    "If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network, as is outlined below.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Figures/activation_function.ppm\" style=\"width:80%\">\n",
    "    <figcaption>Source: Google Images (2019)</figcaption>\n",
    "</figure>\n",
    "\n",
    "Convolutional neural networks are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. They have three main types of layers, which are:\n",
    "- Convolutional layer\n",
    "- Pooling layer\n",
    "- Fully-connected (FC) layer\n",
    "\n",
    "The *convolutional layer* is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or *pooling layers*, the *fully-connected* layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.\n",
    "\n",
    "#### Convolutional layer\n",
    "\n",
    "The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are:\n",
    "\n",
    "- Input data: Since in this projects we are going to work with galaxy images, let’s assume that the input will be a color image. It is made up of a matrix of pixels in 3D, which means that the input will have three dimensions (a height, width, and depth) which correspond to RGB in an image.\n",
    "- Filter: It is the feature detector, also known as a kernel, which will move across the receptive fields of the image, checking if the feature is present. This process is known as a *convolution*. The feature detector is a two-dimensional (2-D) array of weights, which represents part of the image. They can vary in size, which determines the size of the receptive field. The filter is then applied to an area of the image, and a dot product is calculated between the input pixels and the filter. This dot product is then fed into an output array. Afterwards, the filter shifts by a *stride*, repeating the process until the kernel has swept across the entire image.\n",
    "- Feature map: The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature.\n",
    "\n",
    "After each convolution operation, a CNN applies an activation function transformation to the feature map, introducing nonlinearity to the model. \n",
    "\n",
    "As we mentioned earlier, another convolution layer can follow the initial convolution layer. When this happens, the structure of the CNN can become hierarchical as the later layers can see the pixels within the receptive fields of prior layers.\n",
    "\n",
    "<a title=\"Vincent Dumoulin, Francesco Visin, MIT &lt;http://opensource.org/licenses/mit-license.php&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Convolution_arithmetic_-_Padding_strides.gif\"><img width=\"512\" alt=\"Convolution arithmetic - Padding strides\" src=\"https://upload.wikimedia.org/wikipedia/commons/0/04/Convolution_arithmetic_-_Padding_strides.gif\"></a>\n",
    "\n",
    "#### Pooling layer\n",
    "\n",
    "Pooling layers, also known as downsampling, conducts dimensionality reduction, reducing the number of parameters in the input. Its purpose is to gradually shrink the representation’s spatial dimension. Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field, populating the output array. There are two main types of pooling:\n",
    "\n",
    "- Max pooling: As the filter moves across the input, it selects the pixel with the maximum value to send to the output array. As an aside, this approach tends to be used more often compared to average pooling.\n",
    "- Average pooling: As the filter moves across the input, it calculates the average value within the receptive field to send to the output array.\n",
    "\n",
    "<a title=\"Rafay Qayyum - Introduction To Pooling Layers In CNN\" href=\"https://pub.towardsai.net/introduction-to-pooling-layers-in-cnn-dafe61eabe34\"><img width=\"512\" alt=\"Introduction To Pooling Layers In CNN\" src=\"https://miro.medium.com/v2/resize:fit:828/1*fXxDBsJ96FKEtMOa9vNgjA.gif\"></a>\n",
    "\n",
    "While a lot of information is lost in the pooling layer, it also has a number of benefits to the CNN. They help to reduce complexity, improve efficiency, and limit risk of overfitting.\n",
    "\n",
    "#### Fully-connected layer\n",
    "\n",
    "The pixel values of the input image are not directly connected to the output layer in partially connected layers. However, in the fully-connected layer, each node in the output layer connects directly to a node in the previous layer.\n",
    "\n",
    "This kind of layers perform the task of classification based on the features extracted through the previous layers and their different filters. While convolutional and pooling layers tend to use ReLu functions, for example, fully-connected layers usually leverage a softmax activation function to classify inputs appropriately, producing a probability from 0 to 1. This is what we will use in this project.\n",
    "\n",
    "<figure>\n",
    "    <img src=\"Figures/fully_cnn.jpg\" style=\"width:100%\">\n",
    "    <figcaption>Source: https://developersbreach.com/convolution-neural-network-deep-learning/</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06ea23ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- Plot features ------------------------------\n",
    "# Properties to decorate the plots.\n",
    "plt.rcParams['axes.linewidth'] = 0.5\n",
    "plt.rcParams['text.usetex'] = False\n",
    "plt.rcParams['font.family'] = 'serif'   \n",
    "plt.rcParams['font.sans-serif'] = 'New Century Schoolbook' # 'Times', 'Liberation Serif', 'Times New Roman'\n",
    "#plt.rcParams['font.serif'] = ['Helvetica']\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['legend.edgecolor'] = 'k'\n",
    "plt.rcParams['legend.markerscale'] = 7\n",
    "plt.rcParams['xtick.minor.visible'] = True\n",
    "plt.rcParams['ytick.minor.visible'] = True\n",
    "plt.rcParams['xtick.top'] = False\n",
    "plt.rcParams['ytick.right'] = False\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.rcParams['xtick.major.width']= 0.5\n",
    "plt.rcParams['xtick.major.size']= 5.0\n",
    "plt.rcParams['xtick.minor.width']= 0.5\n",
    "plt.rcParams['xtick.minor.size']= 3.0\n",
    "plt.rcParams['ytick.major.width']= 0.5\n",
    "plt.rcParams['ytick.major.size']= 5.0\n",
    "plt.rcParams['ytick.minor.width']= 0.5\n",
    "plt.rcParams['ytick.minor.size']= 3.0\n",
    "# ----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d1c7e8",
   "metadata": {},
   "source": [
    "We need to load the images of the dataset. It means we need to have in memory the *training* and *testing* images. To do this, we will use the `default_loader` function of Torch, which will enable us to convert them to *tensors*, the essential data unit of Torch. In addition, we need to load the *probability distributions* for each image. Let us do this firts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f5f3176",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training_solutions_rev1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27947/904303752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training_solutions_rev1.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'training_solutions_rev1.csv'"
     ]
    }
   ],
   "source": [
    "probabilities = pd.read_csv('training_solutions_rev1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ead8297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 61578 entries, 0 to 61577\n",
      "Data columns (total 38 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   GalaxyID   61578 non-null  int64  \n",
      " 1   Class1.1   61578 non-null  float64\n",
      " 2   Class1.2   61578 non-null  float64\n",
      " 3   Class1.3   61578 non-null  float64\n",
      " 4   Class2.1   61578 non-null  float64\n",
      " 5   Class2.2   61578 non-null  float64\n",
      " 6   Class3.1   61578 non-null  float64\n",
      " 7   Class3.2   61578 non-null  float64\n",
      " 8   Class4.1   61578 non-null  float64\n",
      " 9   Class4.2   61578 non-null  float64\n",
      " 10  Class5.1   61578 non-null  float64\n",
      " 11  Class5.2   61578 non-null  float64\n",
      " 12  Class5.3   61578 non-null  float64\n",
      " 13  Class5.4   61578 non-null  float64\n",
      " 14  Class6.1   61578 non-null  float64\n",
      " 15  Class6.2   61578 non-null  float64\n",
      " 16  Class7.1   61578 non-null  float64\n",
      " 17  Class7.2   61578 non-null  float64\n",
      " 18  Class7.3   61578 non-null  float64\n",
      " 19  Class8.1   61578 non-null  float64\n",
      " 20  Class8.2   61578 non-null  float64\n",
      " 21  Class8.3   61578 non-null  float64\n",
      " 22  Class8.4   61578 non-null  float64\n",
      " 23  Class8.5   61578 non-null  float64\n",
      " 24  Class8.6   61578 non-null  float64\n",
      " 25  Class8.7   61578 non-null  float64\n",
      " 26  Class9.1   61578 non-null  float64\n",
      " 27  Class9.2   61578 non-null  float64\n",
      " 28  Class9.3   61578 non-null  float64\n",
      " 29  Class10.1  61578 non-null  float64\n",
      " 30  Class10.2  61578 non-null  float64\n",
      " 31  Class10.3  61578 non-null  float64\n",
      " 32  Class11.1  61578 non-null  float64\n",
      " 33  Class11.2  61578 non-null  float64\n",
      " 34  Class11.3  61578 non-null  float64\n",
      " 35  Class11.4  61578 non-null  float64\n",
      " 36  Class11.5  61578 non-null  float64\n",
      " 37  Class11.6  61578 non-null  float64\n",
      "dtypes: float64(37), int64(1)\n",
      "memory usage: 17.9 MB\n"
     ]
    }
   ],
   "source": [
    "# Let us get some info of the probabilities dataframe\n",
    "probabilities.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d988e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█████████████▉                                                               | 11103/61578 [05:37<26:08, 32.18it/s]"
     ]
    }
   ],
   "source": [
    "# Create a list to store the image tensors, their IDs and their probabilities\n",
    "image_list = []\n",
    "ID_list = []\n",
    "probabilities_list = []\n",
    "\n",
    "# CenterCrop object to crop the image\n",
    "center_crop = transforms.CenterCrop(256)\n",
    "\n",
    "# Iterate over the images in the folder\n",
    "for filename in tqdm(os.listdir('images_training_rev1')):\n",
    "    # Load each image using the default loader from torchvision\n",
    "    image = default_loader(os.path.join('images_training_rev1', filename))\n",
    "    \n",
    "    # Crop the image to the given dimension\n",
    "    cropped_image = center_crop(image)\n",
    "    \n",
    "    # Convert the image to a tensor\n",
    "    tensor_image = torch.tensor(np.array(cropped_image, dtype=np.single), dtype=torch.float)/255.0\n",
    "    \n",
    "    # Reshape the tensor\n",
    "    tensor_image = tensor_image.reshape(3, 256, 256)\n",
    "    \n",
    "    # Append the tensor to the image list\n",
    "    image_list.append(tensor_image)\n",
    "    \n",
    "    # Append the ID to the ID list\n",
    "    ID_temp = int(filename[:-4])\n",
    "    ID_list.append(ID_temp)\n",
    "    \n",
    "    # Append the probability to the probabilities list\n",
    "    probabilities_list.append(probabilities.loc[probabilities['GalaxyID'] == ID_temp].to_numpy(dtype=np.single).reshape(-1)[1:])\n",
    "\n",
    "# Concatenate the list of image tensors into a single tensor\n",
    "image_list = torch.stack(image_list)\n",
    "# Convert the list of IDs into a Torch tensor of integers\n",
    "ID_list = torch.tensor(ID_list, dtype=torch.int32)\n",
    "# Convert the list of probability distributions into a Torch tensor\n",
    "probabilities_list = torch.tensor(np.array(probabilities_list), dtype=torch.float)\n",
    "print(\"¡Listo!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the dimenions of the tensors\n",
    "print(image_list.shape, \"\\n\")\n",
    "print(ID_list.shape, \"\\n\")\n",
    "print(probabilities_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229a1e9e",
   "metadata": {},
   "source": [
    "We need to define the training and testing dataset. To do so, we will consider the testing set composed of the 15% of the images of the original dataset, while the other 85% will remain as the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5610b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = 0.15\n",
    "testing_size = int(len(ID_list)*prop)\n",
    "training_size = len(ID_list) - testing_size\n",
    "\n",
    "# Check the sizes\n",
    "print(\"Testing size: \", testing_size, \"\\n\")\n",
    "print(\"Training size: \", training_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0af11d4",
   "metadata": {},
   "source": [
    "Let us generate a sequence of random integers that are the indexes of the images and their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ae9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_indexes = np.random.randint(0, len(ID_list) - 1, testing_size)\n",
    "training_indexes = np.delete(np.linspace(0, len(ID_list), len(ID_list), endpoint=False, dtype=int),\n",
    "                             testing_indexes)\n",
    "\n",
    "# Training IDs and images\n",
    "training_images = image_list[training_indexes]\n",
    "ID_training = ID_list[training_indexes]\n",
    "training_probabilities = probabilities_list[training_indexes]\n",
    "# Testing IDs and images\n",
    "testing_images = image_list[testing_indexes]\n",
    "ID_testing = ID_list[testing_indexes]\n",
    "testing_probabilities = probabilities_list[testing_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check one example to see if it was loaded correctly\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10,10))\n",
    "\n",
    "axes[0].imshow(training_images[4].reshape(256, 256, 3))\n",
    "axes[0].set_title(\"Training\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(testing_images[4].reshape(256, 256, 3))\n",
    "axes[1].set_title(\"Testing\")\n",
    "axes[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b348bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us check the dimension of the arrays\n",
    "print(training_images.shape, \"\\n\")\n",
    "print(testing_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b2d18",
   "metadata": {},
   "source": [
    "Once loaded the data needed to train the model, let us define a custom image class to instance the images and probabililites as these objects. This way, we will be able to manipulate the dataset easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b2809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a custom Dataset class to work the images\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, images, ID, probabilities):\n",
    "        \"\"\" The super() builtin returns a proxy object (temporary object of the superclass)\n",
    "        that allows us to access methods of the base class.\"\"\"\n",
    "        super().__init__()\n",
    "        self.images = images                      # Torch tensor\n",
    "        self.ID = ID                              # ID array\n",
    "        self.probabilities = probabilities        # Pandas dataframe\n",
    "        \n",
    "    # We redefine the __len__() method\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    # We redefine the __getitem__() method\n",
    "    def __getitem__(self, i):\n",
    "        image = self.images[i]\n",
    "        probabilities = self.probabilities[i]\n",
    "        return image, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d33524",
   "metadata": {},
   "source": [
    "We have to instantiate two different objects, one for the training images and the other for the testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bde7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train_data = CustomImageDataset(training_images, ID_training, training_probabilities)\n",
    "\n",
    "# Testing\n",
    "test_data = CustomImageDataset(testing_images, ID_testing, testing_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a89dbd5",
   "metadata": {},
   "source": [
    "Once we have loaded the CustomImageDataset objects, it is time to instantiate the Dataloader objects in order to get the proper inputs to the CNN. Also, it is possible to train the CNN in batches if the inputs are Dataloader objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ca0014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the batch\n",
    "batch_size = 50\n",
    "#batch_size = 1000\n",
    "\n",
    "# Training DataLoader object\n",
    "train_dl = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "# Testing DataLoader object\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b527482",
   "metadata": {},
   "source": [
    "We can check if cuda is available for training. The use of cuda optimizes the training process, allowing us to use the different GPUs we have in our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9519b91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu or gpu device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84759605",
   "metadata": {},
   "source": [
    "Now we have loaded the datasets and we transformed them into appropiate Dataloader objects, it is time to define the model we will train to classificate the input galaxies. As said before, the model is a CNN, and the architecture of such a network will be explained right now. First, let us define the *hyperparametrs* of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c7308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a function to compute the width and height of an output convolutional layer\n",
    "def output_size(W, K, P, S):\n",
    "    # W is the input width and height\n",
    "    # K is the kernel size\n",
    "    # P is the padding\n",
    "    # S is the stride\n",
    "    return ((W - K + 2*P)/S) + 1\n",
    "\n",
    "print(output_size(124, 3, 1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a72c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "n_inputs = 12348\n",
    "n_hidden_1 = 6174                 # 12348/2 = 6174\n",
    "n_hidden_2 = 3087                 # 6174/2 = 3087\n",
    "n_outputs = 37                    # Number of the components of the target vector\n",
    "in_channels = 3\n",
    "out_channels_1 = 5\n",
    "out_channels_2 = 7\n",
    "kernel_size_1 = 4\n",
    "kernel_size_2 = 3\n",
    "p_dropout = 0.1                   # Dropout probability\n",
    "lr = 1e-3                         # Learning rate\n",
    "n_epochs = 100                    # Number of epochs\n",
    "#n_epochs = 300                   # Number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6242ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "class Model(nn.Module):\n",
    "    # Define model elements\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Sequence of transformations implemented by the layers of the network\n",
    "        self.cnn = nn.Sequential(\n",
    "            # Convolution layer. Convolution applyed to the input image. Stride = 1 and no padding\n",
    "            nn.Conv2d(in_channels, out_channels_1, kernel_size_1, stride=2),\n",
    "            # Activation function applyed to the convolutioned map\n",
    "            nn.ReLU(),\n",
    "            # Pooling layer. Max pooling function applyed to the ReLU-convolutioned map. No padding\n",
    "            nn.MaxPool2d(kernel_size_1, stride=1),\n",
    "            # Convolution layer. Convolution applyed to the maxpooled layer before. Stride = 1 and no padding\n",
    "            nn.Conv2d(out_channels_1, out_channels_2, kernel_size_2, stride=3, padding=1),\n",
    "            # Activation function applyed to the convolutioned map\n",
    "            nn.ReLU(),\n",
    "            # Flattens a contiguous range of dims into a tensor\n",
    "            nn.Flatten(),\n",
    "            # Linear transformation of the flattened layer\n",
    "            nn.Linear(n_inputs, n_hidden_1),\n",
    "            # Activation function applyed to the convolutioned map\n",
    "            nn.ReLU(),\n",
    "            # Linear transformation applyed to the ReLU-transformed layer\n",
    "            nn.Linear(n_hidden_1, n_hidden_2),\n",
    "            # Rndomly zeroes some of the elements of the input tensor with probability p_dropout\n",
    "            nn.Dropout(p_dropout),\n",
    "            # Activation function applyed to the dropped out layer\n",
    "            nn.ReLU(),\n",
    "            # Linear transformation applyed to the ReLU-transformed layer \n",
    "            nn.Linear(n_hidden_2, n_outputs),\n",
    "            # Softmax function applied to the linear transformed layer\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "    # Method to transform inputs in outputs considering the internal structure of the network\n",
    "    def forward(self, X):\n",
    "        output = self.cnn(X)\n",
    "        return output\n",
    "    \n",
    "# Now we can create a model and send it at once to the device\n",
    "model = Model().to(device)\n",
    "# We can also inspect its parameters using its state_dict() method\n",
    "print(model.state_dict())\n",
    "# We can check the architecture this way\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e276c3",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"cnn_architecture.jpg\" style=\"width:100%\">\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc8b72",
   "metadata": {},
   "source": [
    "Te architecture of the network is based in partially-connected layers (convolutional and pooling layers) and a fully-connected part. In the convolutional part, a *filter* composed of various *kernels* will move across the input, while doing the convolution (to learn more about convolutions see [here](https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1#:~:text=Each%20filter%20in%20a%20convolution,a%20processed%20version%20of%20each.)). There will be one kernel per input channel and one filter per output channel. After each convolution, a ReLU function is applied to all the pixels in the different channels, except when the max pooling operation is applyed. After the last convolution, the 7 channels of 42x42 will be flattened, to enter the fully-connected part of the network. Then, after a series of linear, ReLUs and one dropping transformations, the last layer of 37 components is evaluted using a *softmax* function to get the desired probability distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310647a8",
   "metadata": {},
   "source": [
    "Now, we have to define the function that will perform the training and testing of the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the training function\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    #size = int(len(dataloader.dataset)/1000)\n",
    "    size = int(len(dataloader.dataset)/batch_size)\n",
    "    tmp = []\n",
    "\n",
    "    # We iterate over batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # We calculate the model's prediction\n",
    "        print(X.dtype)\n",
    "        pred = model(X)\n",
    "        # With the model's prediction we calculate the loss function\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # We apply the backpropagation method\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Training progress\n",
    "        loss, current = loss.item(), batch\n",
    "        tmp.append(loss)\n",
    "        print(f\"Actual batch = {current} | Loss = {loss:>7f} | Processed samples: [{current:>2d}/{size:>2d}]\")\n",
    "    \n",
    "    tmp = np.array(tmp)\n",
    "    loss_avg = tmp.sum()/len(tmp)\n",
    "    return loss_avg\n",
    "\n",
    "# We define the test function\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    #size = int(len(dataloader.dataset)/1000)\n",
    "    size = int(len(dataloader.dataset)/batch_size)\n",
    "    test_loss = 0\n",
    "    j = 0\n",
    "    \n",
    "    # To test, we need to deactivate the calculation of the gradients\n",
    "    with torch.no_grad():\n",
    "        # We iterate over batches\n",
    "        for X, y in dataloader:\n",
    "            # Model's prection\n",
    "            pred = model(X)\n",
    "            # Corresponding errors, which we acumulate in a total value\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            j += 1\n",
    "            \n",
    "    # We calculate the total loss and print it\n",
    "    test_loss /= j\n",
    "    print(f\"Test Error: Avg loss = {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7f73c",
   "metadata": {},
   "source": [
    "In order to train the model, we need to instanciate an optimizer object and a loss function object. Let us do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bc4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function object. It is a Medium Squared Error.\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# We instantiate an optimizer. In this case we choose an Adam optimizer.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=1e-08, weight_decay=0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733946f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict size to gain some perspective about the model\n",
    "print(\"Model's state_dict size:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797baca",
   "metadata": {},
   "source": [
    "We will plot the loss function against the epochs, so, we need to save its value after each epoch of training is concluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d4ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a loss array to plot the training loss function and the testing loss function\n",
    "loss_to_plot = []\n",
    "loss_to_plot_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77cbcc",
   "metadata": {},
   "source": [
    "We are ready to train the model. Let us train it during $n_{epochs}$ epochs, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcb4fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We train the model iterating over the different epochs\n",
    "for t in tqdm(range(n_epochs)):\n",
    "    print(f\"Epoch {t+1}\\n=============================================\")\n",
    "    loss_to_plot.append(train_loop(train_dl, model, loss_fn, optimizer))\n",
    "    loss_to_plot_test.append(test_loop(test_dl, model, loss_fn))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8385c35",
   "metadata": {},
   "source": [
    "Since we have our model trained, now we need to write a function able to convert the probability distributions predicted by the network in words. The 11 questions shown at the beggining used to define the decision tree has each one a set of answers. Let us convert the predicted probabilites in those answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657843bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We save both loss functions\n",
    "np.savetxt('loss_to_plot.txt', loss_to_plot)\n",
    "np.savetxt('loss_to_plot_test.txt', loss_to_plot_test)\n",
    "\n",
    "# We choose an image and calculate the corresponding prediction generated by the model\n",
    "for (X, y) in test_dl:\n",
    "    pred_cpu = model(X)\n",
    "    image_cpu = X[7]\n",
    "    target = y\n",
    "    break\n",
    "\n",
    "pred_cpu = pred_cpu[7].detach().numpy()\n",
    "target = target[7].detach().numpy()\n",
    "\n",
    "# We plot the image to be predicted and as a title the corresponding prediction\n",
    "fig, ax = plt.subplots(1, 1, dpi=280)\n",
    "fig.set_size_inches(4.0, 4.0)\n",
    "ax.axis(\"off\")\n",
    "#plt.title(labels_map[np.argmax(pred_cpu)])\n",
    "ax.imshow(image_cpu.squeeze().reshape(256, 256, 3), cmap=\"gray\")\n",
    "\n",
    "print(\"Prediction\", \"\\t\", \"Target\", \"\\n\", \"_________________________\", \"\\n\")\n",
    "for (pred, tar) in zip(pred_cpu, target):\n",
    "    print(pred, \"\\t\", tar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b077b6",
   "metadata": {},
   "source": [
    "Let us plot the loss function as a function of the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc99d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both loss functions\n",
    "lp = np.loadtxt('loss_to_plot.txt')\n",
    "lp_test = np.loadtxt('loss_to_plot_test.txt')\n",
    "\n",
    "# Let us plot both loss functions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7,7), dpi=200)\n",
    "ax.plot([i for i in range(3, n_epochs+1)], lp[2:], color='darkblue', lw=1.5, label='Training error')\n",
    "ax.plot([i for i in range(3, n_epochs+1)], lp_test[2:], ls=':', color='darkblue', lw=1.5, label='Test error')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Average loss function')\n",
    "#ax.set_xticks([0, 75, 150, 225, 300])\n",
    "#ax.set_xticklabels(['0', '75', '150', '225', '300'])\n",
    "#y_ticks = [0.02, 0.025, 0.03, 0.035, 0.04, 0.045]\n",
    "#ax.set_yticks(y_ticks)\n",
    "#ax.set_yticklabels([str(y_ticks[i]) for i in range(len(y_ticks))])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig('loss.jpg', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
