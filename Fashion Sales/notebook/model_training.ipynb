{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f435f420",
   "metadata": {},
   "source": [
    "## Fashion sales project\n",
    "\n",
    "This project aims to gain insights into the fashion market and optimize the selling process by leveraging on the available fashion sales data and using different machine-learning solutions.\n",
    "\n",
    "#### Workflow of the project\n",
    "- Data Collection\n",
    "- Data Checks to perform\n",
    "- Exploratory data analysis\n",
    "- Data Pre-Processing\n",
    "- Model Training\n",
    "- Choose best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01204003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define all the packages needed to carry out the project\n",
    "import time\n",
    "from pickle import dump, load\n",
    "\n",
    "# --- Data visualization and data analysis ---\n",
    "import matplotlib.pyplot as plt\n",
    "#from mlxtend.plotting import plot_decision_regions\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import uniform\n",
    "import pandas as pd\n",
    "#import prince\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# --- Machine learning models ---\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "#from sklearn.neighbors import KNeighborsClassifier\n",
    "#from sklearn.cluster import KMeans\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from mlxtend.classifier import StackingCVClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Used to ignore warnings generated from StackingCVClassifier\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291a8a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data/mock_fashion_data_uk_us.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f9b542",
   "metadata": {},
   "source": [
    "#### 4) Data Pre-processing\n",
    "- Separate data in predictors (indipendent variables) and responses or targets (dependent variables)\n",
    "- Apply the corresponding transformation on each variable\n",
    "- Check correlation between predictors\n",
    "- Split data in training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c54e2c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictors\n",
    "X = df.drop(columns=['Product Name', 'Price'], axis=1)\n",
    "\n",
    "# We will use as a response variable the price of the clothes\n",
    "y = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee832a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Category</th>\n",
       "      <th>Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Count</th>\n",
       "      <th>Style Attributes</th>\n",
       "      <th>Total Sizes</th>\n",
       "      <th>Available Sizes</th>\n",
       "      <th>Color</th>\n",
       "      <th>Purchase History</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fashion Magazines</th>\n",
       "      <th>Fashion Influencers</th>\n",
       "      <th>Season</th>\n",
       "      <th>Time Period Highest Purchase</th>\n",
       "      <th>Customer Reviews</th>\n",
       "      <th>Social Media Comments</th>\n",
       "      <th>feedback</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ralph Lauren</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Bad</td>\n",
       "      <td>1.421706</td>\n",
       "      <td>492</td>\n",
       "      <td>Streetwear</td>\n",
       "      <td>M, L, XL</td>\n",
       "      <td>XL</td>\n",
       "      <td>Green</td>\n",
       "      <td>Medium</td>\n",
       "      <td>24</td>\n",
       "      <td>Vogue</td>\n",
       "      <td>Chiara Ferragni</td>\n",
       "      <td>Fall/Winter</td>\n",
       "      <td>Daytime</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ted Baker</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>1.037677</td>\n",
       "      <td>57</td>\n",
       "      <td>Vintage</td>\n",
       "      <td>M, L, XL</td>\n",
       "      <td>XL</td>\n",
       "      <td>Black</td>\n",
       "      <td>Above Average</td>\n",
       "      <td>61</td>\n",
       "      <td>Glamour</td>\n",
       "      <td>Leandra Medine</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jigsaw</td>\n",
       "      <td>Footwear</td>\n",
       "      <td>Very Bad</td>\n",
       "      <td>3.967106</td>\n",
       "      <td>197</td>\n",
       "      <td>Streetwear</td>\n",
       "      <td>S, M, L</td>\n",
       "      <td>M</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Average</td>\n",
       "      <td>27</td>\n",
       "      <td>Marie Claire</td>\n",
       "      <td>Gigi Hadid</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Nighttime</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander McQueen</td>\n",
       "      <td>Outerwear</td>\n",
       "      <td>Not Good</td>\n",
       "      <td>2.844659</td>\n",
       "      <td>473</td>\n",
       "      <td>Formal</td>\n",
       "      <td>S, M, L</td>\n",
       "      <td>L</td>\n",
       "      <td>Red</td>\n",
       "      <td>Very High</td>\n",
       "      <td>50</td>\n",
       "      <td>Marie Claire</td>\n",
       "      <td>Chiara Ferragni</td>\n",
       "      <td>Fall/Winter</td>\n",
       "      <td>Weekend</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Other</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tommy Hilfiger</td>\n",
       "      <td>Bottoms</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>1.183242</td>\n",
       "      <td>55</td>\n",
       "      <td>Sporty</td>\n",
       "      <td>M, L, XL</td>\n",
       "      <td>S</td>\n",
       "      <td>Green</td>\n",
       "      <td>Above Average</td>\n",
       "      <td>23</td>\n",
       "      <td>Glamour</td>\n",
       "      <td>Song of Style</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Daytime</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Mixed</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Brand   Category Description    Rating  Review Count  \\\n",
       "0       Ralph Lauren   Footwear         Bad  1.421706           492   \n",
       "1          Ted Baker       Tops    Not Good  1.037677            57   \n",
       "2             Jigsaw   Footwear    Very Bad  3.967106           197   \n",
       "3  Alexander McQueen  Outerwear    Not Good  2.844659           473   \n",
       "4     Tommy Hilfiger    Bottoms   Very Good  1.183242            55   \n",
       "\n",
       "  Style Attributes Total Sizes Available Sizes  Color Purchase History  Age  \\\n",
       "0       Streetwear    M, L, XL              XL  Green           Medium   24   \n",
       "1          Vintage    M, L, XL              XL  Black    Above Average   61   \n",
       "2       Streetwear     S, M, L               M   Blue          Average   27   \n",
       "3           Formal     S, M, L               L    Red        Very High   50   \n",
       "4           Sporty    M, L, XL               S  Green    Above Average   23   \n",
       "\n",
       "  Fashion Magazines Fashion Influencers       Season  \\\n",
       "0             Vogue     Chiara Ferragni  Fall/Winter   \n",
       "1           Glamour      Leandra Medine       Winter   \n",
       "2      Marie Claire          Gigi Hadid       Summer   \n",
       "3      Marie Claire     Chiara Ferragni  Fall/Winter   \n",
       "4           Glamour       Song of Style       Spring   \n",
       "\n",
       "  Time Period Highest Purchase Customer Reviews Social Media Comments  \\\n",
       "0                      Daytime            Mixed                 Mixed   \n",
       "1                      Weekend         Negative               Neutral   \n",
       "2                    Nighttime          Unknown              Negative   \n",
       "3                      Weekend          Neutral                 Other   \n",
       "4                      Daytime         Positive                 Mixed   \n",
       "\n",
       "   feedback  \n",
       "0     Other  \n",
       "1     Other  \n",
       "2   Neutral  \n",
       "3     Other  \n",
       "4  Positive  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if it was done correctly\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78f73d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separete between numerical and categorical features\n",
    "num_features = X.select_dtypes(exclude=\"object\").columns\n",
    "cat_features = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "# Instantiate transformers\n",
    "numeric_transformer = StandardScaler()\n",
    "oh_transformer = OneHotEncoder()\n",
    "\n",
    "# Create a Column Transformer with 2 types of transformers\n",
    "preprocessor = ColumnTransformer([(\"OneHotEncoder\", oh_transformer, cat_features),\n",
    "                                  (\"StandardScaler\", numeric_transformer, num_features),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10ad71bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations on predictors\n",
    "X = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c6600b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert transformed predictors into pandas DataFrame\n",
    "testing_X = pd.DataFrame(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6616539e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the correlation matrix\n",
    "correlation_matrix = testing_X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "400f24f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0     Price\n",
      "0      1.000000  0.001596\n",
      "Price  0.001596  1.000000\n",
      "              1     Price\n",
      "1      1.000000  0.002651\n",
      "Price  0.002651  1.000000\n",
      "              2     Price\n",
      "2      1.000000  0.000216\n",
      "Price  0.000216  1.000000\n",
      "              3     Price\n",
      "3      1.000000 -0.000894\n",
      "Price -0.000894  1.000000\n",
      "              4     Price\n",
      "4      1.000000 -0.001711\n",
      "Price -0.001711  1.000000\n",
      "             5    Price\n",
      "5      1.00000  0.00128\n",
      "Price  0.00128  1.00000\n",
      "              6     Price\n",
      "6      1.000000 -0.001819\n",
      "Price -0.001819  1.000000\n",
      "              7     Price\n",
      "7      1.000000 -0.001323\n",
      "Price -0.001323  1.000000\n",
      "              8     Price\n",
      "8      1.000000 -0.000368\n",
      "Price -0.000368  1.000000\n",
      "              9     Price\n",
      "9      1.000000  0.000228\n",
      "Price  0.000228  1.000000\n",
      "             10     Price\n",
      "10     1.000000  0.000739\n",
      "Price  0.000739  1.000000\n",
      "             11     Price\n",
      "11     1.000000 -0.001266\n",
      "Price -0.001266  1.000000\n",
      "            12    Price\n",
      "12     1.00000  0.00115\n",
      "Price  0.00115  1.00000\n",
      "             13     Price\n",
      "13     1.000000 -0.001199\n",
      "Price -0.001199  1.000000\n",
      "             14     Price\n",
      "14     1.000000  0.000664\n",
      "Price  0.000664  1.000000\n",
      "             15     Price\n",
      "15     1.000000 -0.000573\n",
      "Price -0.000573  1.000000\n",
      "             16     Price\n",
      "16     1.000000  0.000885\n",
      "Price  0.000885  1.000000\n",
      "             17     Price\n",
      "17     1.000000 -0.000256\n",
      "Price -0.000256  1.000000\n",
      "             18     Price\n",
      "18     1.000000 -0.000386\n",
      "Price -0.000386  1.000000\n",
      "             19     Price\n",
      "19     1.000000 -0.000012\n",
      "Price -0.000012  1.000000\n",
      "             20     Price\n",
      "20     1.000000  0.000487\n",
      "Price  0.000487  1.000000\n",
      "             21     Price\n",
      "21     1.000000  0.001305\n",
      "Price  0.001305  1.000000\n",
      "             22     Price\n",
      "22     1.000000 -0.001265\n",
      "Price -0.001265  1.000000\n",
      "          23  Price\n",
      "23     1.000  0.002\n",
      "Price  0.002  1.000\n",
      "             24     Price\n",
      "24     1.000000 -0.002118\n",
      "Price -0.002118  1.000000\n",
      "            25    Price\n",
      "25     1.00000 -0.00182\n",
      "Price -0.00182  1.00000\n",
      "             26     Price\n",
      "26     1.000000  0.000399\n",
      "Price  0.000399  1.000000\n",
      "            27    Price\n",
      "27     1.00000  0.00055\n",
      "Price  0.00055  1.00000\n",
      "             28     Price\n",
      "28     1.000000 -0.000585\n",
      "Price -0.000585  1.000000\n",
      "             29     Price\n",
      "29     1.000000  0.000437\n",
      "Price  0.000437  1.000000\n",
      "             30     Price\n",
      "30     1.000000  0.000609\n",
      "Price  0.000609  1.000000\n",
      "             31     Price\n",
      "31     1.000000  0.001526\n",
      "Price  0.001526  1.000000\n",
      "             32     Price\n",
      "32     1.000000  0.001245\n",
      "Price  0.001245  1.000000\n",
      "             33     Price\n",
      "33     1.000000 -0.000725\n",
      "Price -0.000725  1.000000\n",
      "             34     Price\n",
      "34     1.000000 -0.001632\n",
      "Price -0.001632  1.000000\n",
      "           35   Price\n",
      "35     1.0000  0.0003\n",
      "Price  0.0003  1.0000\n",
      "             36     Price\n",
      "36     1.000000  0.000892\n",
      "Price  0.000892  1.000000\n",
      "             37     Price\n",
      "37     1.000000 -0.001191\n",
      "Price -0.001191  1.000000\n",
      "             38     Price\n",
      "38     1.000000  0.000212\n",
      "Price  0.000212  1.000000\n",
      "             39     Price\n",
      "39     1.000000 -0.001325\n",
      "Price -0.001325  1.000000\n",
      "             40     Price\n",
      "40     1.000000  0.000575\n",
      "Price  0.000575  1.000000\n",
      "             41     Price\n",
      "41     1.000000  0.000537\n",
      "Price  0.000537  1.000000\n",
      "             42     Price\n",
      "42     1.000000  0.000117\n",
      "Price  0.000117  1.000000\n",
      "             43     Price\n",
      "43     1.000000  0.000375\n",
      "Price  0.000375  1.000000\n",
      "             44     Price\n",
      "44     1.000000 -0.000005\n",
      "Price -0.000005  1.000000\n",
      "             45     Price\n",
      "45     1.000000 -0.000488\n",
      "Price -0.000488  1.000000\n",
      "             46     Price\n",
      "46     1.000000  0.000323\n",
      "Price  0.000323  1.000000\n",
      "             47     Price\n",
      "47     1.000000  0.000532\n",
      "Price  0.000532  1.000000\n",
      "             48     Price\n",
      "48     1.000000 -0.001444\n",
      "Price -0.001444  1.000000\n",
      "             49     Price\n",
      "49     1.000000 -0.000497\n",
      "Price -0.000497  1.000000\n",
      "             50     Price\n",
      "50     1.000000  0.000668\n",
      "Price  0.000668  1.000000\n",
      "             51     Price\n",
      "51     1.000000  0.000079\n",
      "Price  0.000079  1.000000\n",
      "             52     Price\n",
      "52     1.000000 -0.000551\n",
      "Price -0.000551  1.000000\n",
      "             53     Price\n",
      "53     1.000000  0.001061\n",
      "Price  0.001061  1.000000\n",
      "             54     Price\n",
      "54     1.000000  0.000613\n",
      "Price  0.000613  1.000000\n",
      "             55     Price\n",
      "55     1.000000 -0.000782\n",
      "Price -0.000782  1.000000\n",
      "             56     Price\n",
      "56     1.000000 -0.000985\n",
      "Price -0.000985  1.000000\n",
      "             57     Price\n",
      "57     1.000000  0.001371\n",
      "Price  0.001371  1.000000\n",
      "             58     Price\n",
      "58     1.000000 -0.000844\n",
      "Price -0.000844  1.000000\n",
      "             59     Price\n",
      "59     1.000000  0.000707\n",
      "Price  0.000707  1.000000\n",
      "             60     Price\n",
      "60     1.000000 -0.001566\n",
      "Price -0.001566  1.000000\n",
      "             61     Price\n",
      "61     1.000000  0.001252\n",
      "Price  0.001252  1.000000\n",
      "            62    Price\n",
      "62     1.00000  0.00006\n",
      "Price  0.00006  1.00000\n",
      "             63     Price\n",
      "63     1.000000  0.000936\n",
      "Price  0.000936  1.000000\n",
      "             64     Price\n",
      "64     1.000000 -0.000732\n",
      "Price -0.000732  1.000000\n",
      "             65     Price\n",
      "65     1.000000 -0.000199\n",
      "Price -0.000199  1.000000\n",
      "             66     Price\n",
      "66     1.000000 -0.000789\n",
      "Price -0.000789  1.000000\n",
      "             67     Price\n",
      "67     1.000000  0.000161\n",
      "Price  0.000161  1.000000\n",
      "             68     Price\n",
      "68     1.000000  0.001135\n",
      "Price  0.001135  1.000000\n",
      "             69     Price\n",
      "69     1.000000  0.001134\n",
      "Price  0.001134  1.000000\n",
      "           70   Price\n",
      "70     1.0000 -0.0018\n",
      "Price -0.0018  1.0000\n",
      "             71     Price\n",
      "71     1.000000 -0.001784\n",
      "Price -0.001784  1.000000\n",
      "            72    Price\n",
      "72     1.00000  0.00187\n",
      "Price  0.00187  1.00000\n",
      "             73     Price\n",
      "73     1.000000  0.001493\n",
      "Price  0.001493  1.000000\n",
      "             74     Price\n",
      "74     1.000000 -0.001473\n",
      "Price -0.001473  1.000000\n",
      "             75     Price\n",
      "75     1.000000  0.000055\n",
      "Price  0.000055  1.000000\n",
      "             76     Price\n",
      "76     1.000000  0.000543\n",
      "Price  0.000543  1.000000\n",
      "             77     Price\n",
      "77     1.000000 -0.000745\n",
      "Price -0.000745  1.000000\n",
      "             78     Price\n",
      "78     1.000000 -0.000894\n",
      "Price -0.000894  1.000000\n",
      "             79     Price\n",
      "79     1.000000 -0.001043\n",
      "Price -0.001043  1.000000\n",
      "             80     Price\n",
      "80     1.000000  0.000819\n",
      "Price  0.000819  1.000000\n",
      "             81     Price\n",
      "81     1.000000  0.001319\n",
      "Price  0.001319  1.000000\n",
      "             82     Price\n",
      "82     1.000000 -0.000556\n",
      "Price -0.000556  1.000000\n",
      "             83     Price\n",
      "83     1.000000  0.001407\n",
      "Price  0.001407  1.000000\n",
      "             84     Price\n",
      "84     1.000000 -0.000948\n",
      "Price -0.000948  1.000000\n",
      "             85     Price\n",
      "85     1.000000  0.000021\n",
      "Price  0.000021  1.000000\n",
      "             86     Price\n",
      "86     1.000000  0.000075\n",
      "Price  0.000075  1.000000\n",
      "             87     Price\n",
      "87     1.000000  0.000092\n",
      "Price  0.000092  1.000000\n",
      "           88   Price\n",
      "88     1.0000  0.0015\n",
      "Price  0.0015  1.0000\n",
      "             89     Price\n",
      "89     1.000000 -0.002098\n",
      "Price -0.002098  1.000000\n",
      "             90     Price\n",
      "90     1.000000  0.000105\n",
      "Price  0.000105  1.000000\n",
      "           91   Price\n",
      "91     1.0000  0.0004\n",
      "Price  0.0004  1.0000\n",
      "             92     Price\n",
      "92     1.000000  0.000181\n",
      "Price  0.000181  1.000000\n",
      "            93    Price\n",
      "93     1.00000  0.00077\n",
      "Price  0.00077  1.00000\n",
      "             94     Price\n",
      "94     1.000000 -0.001272\n",
      "Price -0.001272  1.000000\n",
      "             95     Price\n",
      "95     1.000000 -0.000107\n",
      "Price -0.000107  1.000000\n",
      "             96     Price\n",
      "96     1.000000 -0.001745\n",
      "Price -0.001745  1.000000\n",
      "             97     Price\n",
      "97     1.000000  0.002177\n",
      "Price  0.002177  1.000000\n",
      "             98     Price\n",
      "98     1.000000  0.001422\n",
      "Price  0.001422  1.000000\n",
      "             99     Price\n",
      "99     1.000000  0.001396\n",
      "Price  0.001396  1.000000\n",
      "            100     Price\n",
      "100    1.000000 -0.001545\n",
      "Price -0.001545  1.000000\n",
      "            101     Price\n",
      "101    1.000000 -0.000435\n",
      "Price -0.000435  1.000000\n",
      "            102     Price\n",
      "102    1.000000 -0.001569\n",
      "Price -0.001569  1.000000\n",
      "           103    Price\n",
      "103    1.00000  0.00073\n",
      "Price  0.00073  1.00000\n",
      "           104    Price\n",
      "104    1.00000  0.00049\n",
      "Price  0.00049  1.00000\n",
      "            105     Price\n",
      "105    1.000000  0.001202\n",
      "Price  0.001202  1.000000\n",
      "            106     Price\n",
      "106    1.000000 -0.000843\n",
      "Price -0.000843  1.000000\n"
     ]
    }
   ],
   "source": [
    "# Correlation between predictors and response\n",
    "for column in testing_X.columns:\n",
    "    print(pd.concat((testing_X[[column]], y), axis=1).corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8ce8dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of correlation values\n",
    "correlation_values = []\n",
    "\n",
    "# Getting the correlation values from the lower triangle of the correlation matrix\n",
    "for i in range(correlation_matrix.shape[0]):\n",
    "    for j in range(i+1):\n",
    "        correlation_values.append(correlation_matrix.to_numpy()[i,j])\n",
    "        \n",
    "# Convertion to numpy array\n",
    "correlation_values = np.array(correlation_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93bad31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram to see the distribution of correlation values given the huge number of them\n",
    "sns.histplot(correlation_values, bins=20)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc860e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((800000, 107), (200000, 107))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate dataset into training and validation data\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train.shape, X_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca35758",
   "metadata": {},
   "source": [
    "#### 5) Model Training\n",
    "- Choose the models to be used\n",
    "- Perform a hyperparameter tunning\n",
    "- Save the best-fit hyperparameters\n",
    "- Choose the best-fit hyperparameters to train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf4cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models. Each value corresponds to a list whose firts element is the learning algorithm and the second one is\n",
    "# the dictionary of values to perform the randomized search\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(n_jobs = 8),\n",
    "    \"Lasso\": [Lasso(), dict(alpha = np.logspace(-2, 2, 100, dtype=np.float32), \n",
    "                            max_iter = np.arange(500, 2000, 1, dtype=np.int32), \n",
    "                            tol = np.logspace(-6, -3, 100, dtype=np.float32),\n",
    "                            warm_start = [True, False])],\n",
    "    \"Ridge\": [Ridge(), dict(alpha = np.logspace(-2, 2, 100, dtype=np.float32),\n",
    "                            tol = np.logspace(-6, -3, 100, dtype=np.float32),\n",
    "                            solver = ['auto', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs'])],\n",
    "    \"K-Neighbors Regressor\": [KNeighborsRegressor(n_jobs = 8), dict(n_neighbors = np.arange(3, 100, 1, dtype=np.int32),\n",
    "                                                                  weights = ['uniform', 'distance'],\n",
    "                                                                  algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                                                                  leaf_size = np.arange(20, 75, 1, dtype=np.int32))],\n",
    "    \"Decision Tree Regressor\": [DecisionTreeRegressor(random_state = 10), dict(criterion = ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'],\n",
    "                                                                               max_depth = np.arange(1, 10, 1, dtype=np.int32),\n",
    "                                                                               min_samples_split = np.linspace(0.05, 1.0, 20),\n",
    "                                                                               min_samples_leaf = np.linspace(0.05, 0.5, 20),\n",
    "                                                                               max_features = np.arange(1, X_train.shape[1], 1, dtype=np.int32),\n",
    "                                                                               max_leaf_nodes = np.arange(2, 10, 1, dtype=np.int32),\n",
    "                                                                               min_impurity_decrease = [0.0, 0.5, 1.0])],\n",
    "    \"Random Forest Regressor\": [RandomForestRegressor(n_jobs = 8, random_state = 10), dict(n_estimators = np.arange(50, 1000, 1, dtype=np.int32),\n",
    "                                                                      criterion = ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],\n",
    "                                                                      max_depth = np.arange(1, 10, 1, dtype=np.int32),\n",
    "                                                                      min_samples_split = np.linspace(0.05, 1.0, 20),\n",
    "                                                                      min_samples_leaf = np.linspace(0.05, 1.0, 20),\n",
    "                                                                      max_features = np.arange(1, X_train.shape[1], 1, dtype=np.int32),\n",
    "                                                                      max_leaf_nodes = np.arange(1, 10, 1, dtype=np.int32),\n",
    "                                                                      min_impurity_decrease = [0.0, 0.5, 1.0],\n",
    "                                                                      bootstrap = [True, False],\n",
    "                                                                      ccp_alpha = np.linspace(0.0, 0.3, 20),\n",
    "                                                                      max_samples = np.linspace(0.05, 1.0, 20))],\n",
    "    \"XGBRegressor\": [XGBRegressor(n_jobs = 8, random_state = 10, device = 'cpu', eval_metric = r2_score), dict(n_estimators = np.arange(50, 1000, 1, dtype=np.int32),\n",
    "                                          max_depth = np.arange(1, 10, 1, dtype=np.int32),\n",
    "                                          max_leaves = np.arange(0, 5000, 1, dtype=np.int32),\n",
    "                                          grow_policy = ['depthwise', 'lossguide'],\n",
    "                                          learning_rate = np.logspace(-3, 2, 60),\n",
    "                                          booster = ['gbtree', 'gblinear', 'dart'],\n",
    "                                          reg_alpha = np.logspace(-3, 2, 60),\n",
    "                                          reg_lambda = np.logspace(-3, 2, 60),\n",
    "                                          colsample_bytree = np.linspace(0.05, 1.0, 20))],\n",
    "    \"CatBoosting Regressor\": [CatBoostRegressor(verbose=False, loss_function = 'RMSE', random_state = 10), dict(learning_rate = np.logspace(-3, 2, 60),\n",
    "                                                                     max_depth = np.arange(1, 10, 1, dtype=np.int32))],\n",
    "    \"AdaBoost Regressor\": [AdaBoostRegressor(random_state = 10), dict(n_estimators = np.arange(25, 200, 1, dtype=np.int32),\n",
    "                                                                      learning_rate = np.logspace(-3, 1, 50),\n",
    "                                                                      loss = ['linear', 'square', 'exponential'])]\n",
    "}\n",
    "\n",
    "# Some metrics for XGBRegressor: mean_squared_error, mean_absolute_error, mean_gamma_deviance, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "343fbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary with the RandomizedSearchCV object's instance for each model\n",
    "randomized_search_objects = {\"Lasso\": RandomizedSearchCV(models[\"Lasso\"][0], models[\"Lasso\"][1], random_state = 1),\n",
    "                             \"Ridge\": RandomizedSearchCV(models[\"Ridge\"][0], models[\"Ridge\"][1], random_state = 2),\n",
    "                             \"K-Neighbors Regressor\": RandomizedSearchCV(models[\"K-Neighbors Regressor\"][0], models[\"K-Neighbors Regressor\"][1], random_state = 3),\n",
    "                             \"Decision Tree Regressor\": RandomizedSearchCV(models[\"Decision Tree Regressor\"][0], models[\"Decision Tree Regressor\"][1], random_state = 4),\n",
    "                             \"Random Forest Regressor\": RandomizedSearchCV(models[\"Random Forest Regressor\"][0], models[\"Random Forest Regressor\"][1], random_state = 5),\n",
    "                             \"XGBRegressor\": RandomizedSearchCV(models[\"XGBRegressor\"][0], models[\"XGBRegressor\"][1], random_state = 6),\n",
    "                             \"CatBoosting Regressor\": RandomizedSearchCV(models[\"CatBoosting Regressor\"][0], models[\"CatBoosting Regressor\"][1], random_state = 7),\n",
    "                             \"AdaBoost Regressor\": RandomizedSearchCV(models[\"AdaBoost Regressor\"][0], models[\"AdaBoost Regressor\"][1], random_state = 8)}\n",
    "\n",
    "best_fit_objects = {\"Lasso\": None,\n",
    "                    \"Ridge\": None,\n",
    "                    \"K-Neighbors Regressor\": None,\n",
    "                    \"Decision Tree Regressor\": None,\n",
    "                    \"Random Forest Regressor\": None,\n",
    "                    \"XGBRegressor\": None,\n",
    "                    \"CatBoosting Regressor\": None,\n",
    "                    \"AdaBoost Regressor\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7477d6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of finding the best hyperparameters finished. It took:  0.8125252405802409 min\n",
      "{'warm_start': False, 'tol': 0.00017475284, 'max_iter': 1054, 'alpha': 0.49770236}\n"
     ]
    }
   ],
   "source": [
    "# Search of the best-fit hyperparameters using Lasso algorithm\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"Lasso\"] = randomized_search_objects[\"Lasso\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"Lasso\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/lasso_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"Lasso\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec639d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of finding the best hyperparameters finished. It took:  0.25815764665603635 min\n",
      "{'tol': 1e-04, 'solver': 'sag', 'alpha': 3.5111918}\n"
     ]
    }
   ],
   "source": [
    "# Search of the best fit hyperparameters using Ridge algorithm\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"Ridge\"] = randomized_search_objects[\"Ridge\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"Ridge\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/ridge_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"Ridge\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e99cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search of the best fit hyperparameters using K-Neighbors Regressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"K-Neighbors Regressor\"] = randomized_search_objects[\"K-Neighbors Regressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"K-Neighbors Regressor\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/knn_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"K-Neighbors Regressor\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91f22434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of finding the best hyperparameters finished. It took:  294.478679060936 min\n",
      "{'min_samples_split': 0.6, 'min_samples_leaf': 0.5, 'min_impurity_decrease': 0.5, 'max_leaf_nodes': 8, 'max_features': 54, 'max_depth': 2, 'criterion': 'squared_error'}\n"
     ]
    }
   ],
   "source": [
    "# Search of the best fit hyperparameters using Decision Tree Regressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"Decision Tree Regressor\"] = randomized_search_objects[\"Decision Tree Regressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"Decision Tree Regressor\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/decision_tree_regressor_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"Decision Tree Regressor\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a16dc21",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1232/2023712604.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Search of the best fit hyperparameters using Random Forest Regressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_fit_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Random Forest Regressor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomized_search_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Random Forest Regressor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process of finding the best hyperparameters finished. It took: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1764\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1766\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1767\u001b[0m             ParameterSampler(\n\u001b[1;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    862\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 782\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    783\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    443\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    451\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Search of the best fit hyperparameters using Random Forest Regressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"Random Forest Regressor\"] = randomized_search_objects[\"Random Forest Regressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"Random Forest Regressor\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/random_forest_regressor_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"Random Forest Regressor\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b156816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search of the best fit hyperparameters using XGBRegressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"XGBRegressor\"] = randomized_search_objects[\"XGBRegressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5198b41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n",
      "learning rate is greater than 1. You probably need to decrease learning rate.\n"
     ]
    },
    {
     "ename": "CatBoostError",
     "evalue": "catboost/private/libs/algo/tensor_search_helpers.cpp:99: This should be unreachable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6237/2152509592.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Search of the best fit hyperparameters using CatBoosting Regressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mt_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbest_fit_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CatBoosting Regressor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandomized_search_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CatBoosting Regressor\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process of finding the best hyperparameters finished. It took: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'loss_function'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5872\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5873\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5874\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5875\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m                 self._train(\n\u001b[0m\u001b[1;32m   2411\u001b[0m                     \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m                     \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env-ml/lib/python3.10/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1790\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1791\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCatBoostError\u001b[0m: catboost/private/libs/algo/tensor_search_helpers.cpp:99: This should be unreachable"
     ]
    }
   ],
   "source": [
    "# Search of the best fit hyperparameters using CatBoosting Regressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"CatBoosting Regressor\"] = randomized_search_objects[\"CatBoosting Regressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"CatBoosting Regressor\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/cat_boosting_regressor_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"CatBoosting Regressor\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11aeed7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process of finding the best hyperparameters finished. It took:  239.4961965084076 min\n",
      "{'n_estimators': 39, 'loss': 'linear', 'learning_rate': 0.15998587196060574}\n"
     ]
    }
   ],
   "source": [
    "# Search of the best fit hyperparameters using AdaBoost Regressor\n",
    "t_i = time.time()\n",
    "best_fit_objects[\"AdaBoost Regressor\"] = randomized_search_objects[\"AdaBoost Regressor\"].fit(X_train, y_train)\n",
    "print(\"Process of finding the best hyperparameters finished. It took: \", (time.time() - t_i)/60, \"min\")\n",
    "\n",
    "# Print the best-fit hyperparameters\n",
    "print(best_fit_objects[\"AdaBoost Regressor\"].best_params_)\n",
    "\n",
    "# Save the best-fit hyperparameters\n",
    "try:\n",
    "    file = open('best_fit_models/ada_boost_regressor_bestfit.pkl', 'wb')\n",
    "    dump(best_fit_objects[\"AdaBoost Regressor\"].best_params_, file)\n",
    "    file.close()\n",
    "  \n",
    "except: \n",
    "    print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "444120e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of optimized models\n",
    "optimized_models = {\"Lasso\": ['best_fit_models/lasso_bestfit.pkl', None],\n",
    "                    \"Ridge\": ['best_fit_models/ridge_bestfit.pkl', None],\n",
    "                    \"Decision Tree Regressor\": ['best_fit_models/decision_tree_regressor_bestfit.pkl', None],\n",
    "                    \"AdaBoost Regressor\": ['best_fit_models/ada_boost_regressor_bestfit.pkl', None]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c6e8737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the best fit hyperparameters\n",
    "for model_name in optimized_models.keys():\n",
    "    try:\n",
    "        file = open(optimized_models[model_name][0], 'rb')\n",
    "        optimized_models[model_name][1] = load(file)\n",
    "        file.close()\n",
    "    \n",
    "    except: \n",
    "        print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a14a17",
   "metadata": {},
   "source": [
    "Now, it is time to train the betsfited models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce1ae8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of models to be trained. Each value corresponds to the learning algorithm with the parameters defining them\n",
    "regressors = {\"Linear Regression\": LinearRegression(n_jobs = 8),\n",
    "          \"Lasso\": Lasso(alpha = optimized_models[\"Lasso\"][1][\"alpha\"],\n",
    "                         max_iter = optimized_models[\"Lasso\"][1][\"max_iter\"],\n",
    "                         tol = optimized_models[\"Lasso\"][1][\"tol\"],\n",
    "                         warm_start = optimized_models[\"Lasso\"][1][\"warm_start\"]),\n",
    "          \"Ridge\": Ridge(alpha = optimized_models[\"Ridge\"][1][\"alpha\"],\n",
    "                         solver = optimized_models[\"Ridge\"][1][\"solver\"],\n",
    "                         tol = optimized_models[\"Ridge\"][1][\"tol\"]),\n",
    "          \"Decision Tree Regressor\": DecisionTreeRegressor(min_samples_split = optimized_models[\"Decision Tree Regressor\"][1][\"min_samples_split\"],\n",
    "                                                           min_samples_leaf = optimized_models[\"Decision Tree Regressor\"][1][\"min_samples_leaf\"],\n",
    "                                                           min_impurity_decrease = optimized_models[\"Decision Tree Regressor\"][1][\"min_impurity_decrease\"],\n",
    "                                                           max_leaf_nodes = optimized_models[\"Decision Tree Regressor\"][1][\"max_leaf_nodes\"],\n",
    "                                                           max_features = optimized_models[\"Decision Tree Regressor\"][1][\"max_features\"],\n",
    "                                                           max_depth = optimized_models[\"Decision Tree Regressor\"][1][\"max_depth\"],\n",
    "                                                           criterion = optimized_models[\"Decision Tree Regressor\"][1][\"criterion\"]),\n",
    "          \"AdaBoost Regressor\": AdaBoostRegressor(learning_rate = optimized_models[\"AdaBoost Regressor\"][1][\"learning_rate\"],\n",
    "                                                  loss = optimized_models[\"AdaBoost Regressor\"][1][\"loss\"],\n",
    "                                                  n_estimators = optimized_models[\"AdaBoost Regressor\"][1][\"n_estimators\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f859ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in regressors:\n",
    "    # Get regressor\n",
    "    regressor = regressors[key]\n",
    "    \n",
    "    # Fit regressor\n",
    "    regressor.fit(X_train, y_train)\n",
    "        \n",
    "    # Save fitted regressor\n",
    "    regressors[key] = regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6d4c2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions dictionary\n",
    "predictions = {\"Linear Regression\": None,\n",
    "               \"Lasso\": None,\n",
    "               \"Ridge\": None,\n",
    "               \"Decision Tree Regressor\": None,\n",
    "               \"AdaBoost Regressor\": None}\n",
    "\n",
    "for key in regressors:\n",
    "    # Make a prediction on test set\n",
    "    predictions[key] = regressors[key].predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab86d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(true, predicted):\n",
    "    mae = mean_absolute_error(true, predicted)\n",
    "    mse = mean_squared_error(true, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true, predicted)\n",
    "    return mae, mse, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "70cc7d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance of Linear Regression for Test set\n",
      "- Mean Absolute Error: 22.4927\n",
      "- Mean Squared Error: 674.4018\n",
      "- Root Mean Squared Error: 25.9692\n",
      "- R2 Score: -0.0002\n",
      "\n",
      "Model performance of Lasso for Test set\n",
      "- Mean Absolute Error: 22.4914\n",
      "- Mean Squared Error: 674.2905\n",
      "- Root Mean Squared Error: 25.9671\n",
      "- R2 Score: -0.0000\n",
      "\n",
      "Model performance of Ridge for Test set\n",
      "- Mean Absolute Error: 22.4927\n",
      "- Mean Squared Error: 674.4018\n",
      "- Root Mean Squared Error: 25.9692\n",
      "- R2 Score: -0.0002\n",
      "\n",
      "Model performance of Decision Tree Regressor for Test set\n",
      "- Mean Absolute Error: 22.4914\n",
      "- Mean Squared Error: 674.2905\n",
      "- Root Mean Squared Error: 25.9671\n",
      "- R2 Score: -0.0000\n",
      "\n",
      "Model performance of AdaBoost Regressor for Test set\n",
      "- Mean Absolute Error: 22.4913\n",
      "- Mean Squared Error: 674.2972\n",
      "- Root Mean Squared Error: 25.9672\n",
      "- R2 Score: -0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in predictions:\n",
    "    model_test_mae, model_test_mse, model_test_rmse, model_test_r2 = evaluate_model(y_validation.to_numpy(), predictions[key])\n",
    "    print('Model performance of ' + key + ' for Test set')\n",
    "    print(\"- Mean Absolute Error: {:.4f}\".format(model_test_mae))\n",
    "    print(\"- Mean Squared Error: {:.4f}\".format(model_test_mse))\n",
    "    print(\"- Root Mean Squared Error: {:.4f}\".format(model_test_rmse))\n",
    "    print(\"- R2 Score: {:.4f}\\n\".format(model_test_r2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
